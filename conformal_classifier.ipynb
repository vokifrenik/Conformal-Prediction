{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aaafec2",
   "metadata": {},
   "source": [
    "Import libraries and useful functions. Code reused from week 4 exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac595401",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn import metrics\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def accuracy(target, pred):\n",
    "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "def compute_confusion_matrix(target, pred, normalize=None, n_classes=10):\n",
    "    return metrics.confusion_matrix(\n",
    "        target.detach().cpu().numpy(),\n",
    "        pred.detach().cpu().numpy(),\n",
    "        labels=np.arange(n_classes),\n",
    "        normalize=normalize\n",
    "    )\n",
    "\n",
    "def show_image(img):\n",
    "    img = img.detach().cpu()\n",
    "    img = img / 2 + 0.5   # unnormalize\n",
    "    with sns.axes_style(\"white\"):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img.permute((1, 2, 0)).numpy())\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24bfce1",
   "metadata": {},
   "source": [
    "Load the CIFAR-10 dataset and split it into train, validation, calibration and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of torchvision datasets are PIL images in the range [0, 1]. \n",
    "# We transform them to PyTorch tensors and rescale them to be in the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Load full training dataset (50,000 samples)\n",
    "full_train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Split into train (40k), validation (5k) and calibration (10k) sets\n",
    "train_size = 35000  # 35,000\n",
    "valid_size = 5000 # 5,000\n",
    "calib_size = 10000 # 10,000\n",
    "\n",
    "train_set, valid_set, calib_set = torch.utils.data.random_split(\n",
    "    full_train_set, [train_size, valid_size, calib_size]\n",
    ")\n",
    "\n",
    "# Test set remains unchanged\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "calib_loader = DataLoader(calib_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Map from class index to class name.\n",
    "classes = {index: name for name, index in full_train_set.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data\")\n",
    "print(\"Number of points:\", len(train_set))\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
    "print(f\"Number of distinct labels: {len(set(train_set.dataset.targets))} (unique labels: {set(train_set.dataset.targets)})\")\n",
    "\n",
    "print(\"\\nValidation data\")\n",
    "print(\"Number of points:\", len(valid_set))\n",
    "x, y = next(iter(valid_loader))\n",
    "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
    "print(f\"Number of distinct labels: {len(set(valid_set.dataset.targets))} (unique labels: {set(valid_set.dataset.targets)})\")\n",
    "\n",
    "print(\"\\nCalibration data\")\n",
    "print(\"Number of points:\", len(calib_set))\n",
    "x, y = next(iter(calib_loader))\n",
    "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
    "print(f\"Number of distinct labels: {len(set(calib_set.dataset.targets))} (unique labels: {set(calib_set.dataset.targets)})\")\n",
    "\n",
    "print(\"\\nTest data\")\n",
    "print(\"Number of points:\", len(test_set))\n",
    "x, y = next(iter(test_loader))\n",
    "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
    "print(f\"Number of distinct labels: {len(set(test_set.targets))} (unique labels: {set(test_set.targets)})\")\n",
    "\n",
    "n_classes = len(set(test_set.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random training images and show them.\n",
    "images, labels = next(iter(train_loader))\n",
    "show_image(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b589ea",
   "metadata": {},
   "source": [
    "Define model architecture and forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.activation_fn = nn.ReLU\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # CNN\n",
    "            nn.Conv2d(3, 32, 5),\n",
    "            self.activation_fn(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            self.activation_fn(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.Dropout(0.2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # FFNN\n",
    "            nn.LazyLinear(128),\n",
    "            self.activation_fn(),\n",
    "            nn.Linear(128, 128),\n",
    "            self.activation_fn(),\n",
    "            nn.Linear(128, 128),\n",
    "            self.activation_fn(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "\n",
    "#model = Model(n_classes)\n",
    "#device = torch.device('cpu')  \n",
    "#model.to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1d593",
   "metadata": {},
   "source": [
    "Initialize grid for grid search and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define grid\n",
    "param_grid = {\n",
    "    \"optimizer\": [\"Adam\", \"SGD\"],\n",
    "    \"lr\": [1e-1, 1e-3, 1e-5]\n",
    "    }\n",
    "\n",
    "# Create iterable list of param combinations\n",
    "keys = param_grid.keys()\n",
    "values = param_grid.values()\n",
    "\n",
    "combinations = list(product(*values))\n",
    "print(combinations)\n",
    "\n",
    "for combo in combinations:\n",
    "    print(combo[0])\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cef40d",
   "metadata": {},
   "source": [
    "Perform grid search cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ad195",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8 ## train model for longer\n",
    "accuracy_every_steps = 500\n",
    "\n",
    "best_accuracy = -np.inf\n",
    "best_param = []\n",
    "\n",
    "train_accuracies = []\n",
    "\n",
    "# Loop over parameter combinations\n",
    "for combo in combinations:\n",
    "    model = Model(n_classes)\n",
    "    device = torch.device('cpu')  \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    step = 0\n",
    "    valid_accuracies = []\n",
    "\n",
    "    # Retrieve learning rate\n",
    "    lr = combo[1]\n",
    "    \n",
    "    # Retrive optimizer\n",
    "    if combo[0] == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr) \n",
    "        \n",
    "    # Train model\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_accuracies_batches = []\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass, compute gradients, perform one training step.\n",
    "\n",
    "            # Compute outputs\n",
    "            output = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(output, targets)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "\n",
    "            # One step of gradient descent\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Increment step counter\n",
    "            step += 1\n",
    "            \n",
    "            # Compute accuracy.\n",
    "            predictions = output.max(1)[1]\n",
    "            train_accuracies_batches.append(accuracy(targets, predictions))\n",
    "            \n",
    "            # Compute trian accuracy every  steps.\n",
    "            if step % accuracy_every_steps == 0:\n",
    "                train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "                print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
    "\n",
    "    # Perfrom cross validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "            # Forward pass, compute gradients, perform one training step.\n",
    "\n",
    "            # Compute outputs\n",
    "            output = model(inputs)\n",
    "                \n",
    "            # Increment step counter\n",
    "            step += 1\n",
    "                \n",
    "            # Compute accuracy.\n",
    "            predictions = output.max(1)[1]\n",
    "            valid_accuracies.append(accuracy(targets, predictions))\n",
    "\n",
    "        mean_valid_accuracy = np.mean(valid_accuracies)\n",
    "        print(f\"Validation accuracy for combo {combo}: {mean_valid_accuracy}\")\n",
    "\n",
    "        if mean_valid_accuracy > best_accuracy:\n",
    "            best_accuracy = mean_valid_accuracy\n",
    "            best_param = combo\n",
    "\n",
    "\n",
    "print(\"Finished training.\")\n",
    "print(\"Best params:\", best_param)\n",
    "print(\"Best accuract:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497ca5f",
   "metadata": {},
   "source": [
    "Train the model using best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = best_param[1]\n",
    "if best_param[0] == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8 ## train model for longer\n",
    "accuracy_every_steps = 500\n",
    "\n",
    "step = 0\n",
    "\n",
    "model = Model(n_classes)\n",
    "device = torch.device('cpu')  \n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass, compute gradients, perform one training step.\n",
    "\n",
    "        # Compute outputs\n",
    "        output = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # One step of gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Increment step counter\n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions = output.max(1)[1]\n",
    "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
    "        \n",
    "        # Validate every `validation_every_steps` steps.\n",
    "        if step % accuracy_every_steps == 0:\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
    "\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ebe47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformal_prediction(calib_loader, model):\n",
    "    true_label_prob = []\n",
    "    alpha = 0.1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in calib_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            output = model(inputs)\n",
    "\n",
    "            # Compute softmax probabilities\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "\n",
    "            # Add softmax probabilities for true labels into a list\n",
    "            for sample in range(len(probs)):\n",
    "                true_label_prob.append(probs[sample][targets[sample].item()])\n",
    "\n",
    "        # Compute non-conformity scores\n",
    "        non_comf_scores = np.zeros(len(true_label_prob))\n",
    "        for i in range(len(true_label_prob)):\n",
    "            non_comf_scores[i] = 1 - true_label_prob[i]\n",
    "        non_comf_scores.sort()\n",
    "            \n",
    "        # Compute threshold\n",
    "        threshold_index = np.ceil((len(non_comf_scores) + 1) * (1 - alpha)) - 1\n",
    "        threshold = non_comf_scores[int(threshold_index)]\n",
    "        print(threshold)\n",
    "\n",
    "    return threshold\n",
    "    \n",
    "thresh = conformal_prediction(calib_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911804f5",
   "metadata": {},
   "source": [
    "Evaluate model performance on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d88ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test set\n",
    "confusion_matrix = np.zeros((n_classes, n_classes))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_accuracies = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output = model(inputs)\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Prediction set per batch\n",
    "        pred_set = []\n",
    "\n",
    "        # Add classes to prediction set\n",
    "        for i in range(len(probs)):\n",
    "            for j in range(len(probs[i])):\n",
    "                if probs[i][j] >= 1 - thresh:\n",
    "                    pred_set.append(j)\n",
    "\n",
    "            print(\"possible classes\", pred_set)\n",
    "            print(\"true class\", targets[i].item())\n",
    "            pred_set = []\n",
    "\n",
    "            ## ADD OTHER DIAGNOSTICS FOR PROPER EVALUATION OF THE CONFORMAL PREDICTIONS\n",
    "        \n",
    "\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        predictions = output.max(1)[1]\n",
    "\n",
    "        # Multiply by len(inputs) because the final batch of DataLoader may be smaller (drop_last=True).\n",
    "        test_accuracies.append(accuracy(targets, predictions) * len(inputs))\n",
    "        \n",
    "        confusion_matrix += compute_confusion_matrix(targets, predictions)\n",
    "\n",
    "    test_accuracy = np.sum(test_accuracies) / len(test_set)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d2986",
   "metadata": {},
   "source": [
    "UPDATE PLOTS TO INCLUDE CONFORMAL PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6c43b",
   "metadata": {},
   "source": [
    "Plot confusion matrix for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(matrix, axis):\n",
    "    axis = {'true': 1, 'pred': 0}[axis]\n",
    "    return matrix / matrix.sum(axis=axis, keepdims=True)\n",
    "\n",
    "x_labels = [classes[i] for i in classes]\n",
    "y_labels = x_labels\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(\n",
    "    ax=plt.gca(),\n",
    "    data=normalize(confusion_matrix, 'true'),\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cmap=\"Reds\",\n",
    "    cbar=False,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=x_labels,\n",
    "    yticklabels=y_labels,\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3acdd37",
   "metadata": {},
   "source": [
    "Plot per class accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('whitegrid'):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=x_labels, y=np.diag(normalize(confusion_matrix, 'true')))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Per-class accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c99ec47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
